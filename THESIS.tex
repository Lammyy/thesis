% This goes at the from of the file - you can select different things here like 12pt, 11pt, paper size, double sided etc.

\documentclass[a4paper,12pt]{article}

% Packages for different settings - there are many of these you can access by googling (item you want and latex).

\usepackage{amsmath} % to create matrices, you should use this package
\usepackage{amsfonts}
\usepackage{pifont}
\usepackage{amsmath}
\usepackage{epsfig}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{color}
\usepackage{amscd}
\usepackage{natbib}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{lscape}
\usepackage{float}
\usepackage[ ]{algorithm2e}
\usepackage{boondox-cal}
\usepackage{bm}
\usepackage{fancyhdr}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
%\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% paper margins settings.
\pagestyle{fancy}  
\pagenumbering{gobble} 
\oddsidemargin0cm
\hoffset-1cm
\voffset-0.5cm
\topmargin-1.4cm 
\textheight25cm \textwidth18cm \parindent0.5cm
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\fancyhead{}
\renewcommand{\headrulewidth}{0pt}
\lhead{Alexander Lam}
\rhead{z5061427}
\begin{document}
\section{Introduction}
\subsection{Problem Statement}
In machine learning, particularly for high dimensional applications such as image analysis, it is often desirable to build generative models, so that we can represent the data in lower dimensions via representation learning, and generate new data similar to the examples in our dataset. Assume our dataset $X=\{x^{(i)}\}^N_{i=1}$ is $N$ i.i.d. samples of random variables $x$. Also assume $x$ can be generated by a stochastic process from a latent continuous random variable $z$. These models involve mapping the dataset x to lower dimensional latent prior $z$ (e.g. $z\sim N(\mu,\Sigma)$) then simulating from the prior $p_\theta(z)$ to generate new data through a decoder $p_\theta(x|z)$; $\theta$ represents the parameters of the distribution, typically by a neural network. In this particular field, there are three main problems to solve:
\begin{enumerate}
\item Estimation of $\theta$, so that we can actually generate new data $x$
\item Evaluation of the posterior density $p(z|x) = \frac{p(z)p(x|z)}{p(x)} = \frac{p(x|z)p(z)}{\int_z p(x,z)dz}$, so we can encode our data $x$ in an efficient representation $z$
\item Marginal inference of $x$ ie. evaluating $p(x)$, so it can be used as a prior for other tasks
\end{enumerate}
However, with high dimensional data, calculating $p(x)$ by marginalising over z is intractable, meaning typical methods such as MCMC or expectation maximization can be computationally expensive or time-consuming. Also, with a large dataset, batch optimization such as Monte Carlo Expectation Maximization is too slow. To circumvent this problem, we apply implicit variational inference, estimating the true posterior $p_\theta(z|x$) with another implicit distribution $q_\phi(z|x)$, a stochastic generator/encoder parametrized by another neural network: given data $x$ it outputs a distribution representing $z$ that could have generated the $x$. We then want to minimize the KL divergence between the two distributions: \[KL(q(z|x)||p(z|x)) = E_{q(z|x)}\log \frac{q(z|x)}{p(z|x)}\] Applying Bayes' law to $p(z|x)$, the KL divergence becomes \[KL(q(z|x)||p(z|x)) = E_{q(z|x)}\left[ \log \frac{q(z|x)}{p(x|z)p(z)}+\log p(x)\right]\] 
Again, $p(x)$ is intractable, so we cannot simply calculate the KL divergence. However, since $\log p(x)$ is constant, we can rearrange the expression to form: \[\log p(x) = KL(q(z|x)||p(z|x)) - E_{q(z|x)}\log \frac{q(z|x)}{p(x|z)p(z)},\]and we can therefore minimize the KL divergence by minimizing the rightmost term, which we call the evidence lower bound: \[ELBO=E_{q(z|x)}\log \frac{q(z|x)}{p(x|z)p(z)}=-KL(q(z|x)||p(z))+E_{q(z|x)}\log p(x|z)\] via back-propagation and stochastic gradient descent. The KL divergence acts as a regularizer whilst the likelihood term represents the reconstruction error. $E_{q(z|x)}\log p(x|z)$ is the probability density of $x$ under the model given $z$ (I'm not entirely sure how to estimate it but I think we sample a few $x$'s from the dataset, and generate $z$'s with $q(z|x)$ then find the mean of something??).
\newpage
We have defined $q(z|x)$ as an implicit distribution: a black box stochastic process parameterized by a neural network. The advantage of using an implicit posterior approximation as opposed to a typical explicit exponential distribution is the ability to model any dependencies within the data (normally $q(z|x) = N(\mu(x),\Sigma(x))$ where $\Sigma(x)$ is a diagonal matrix). The ELBO optimization process involves estimating $KL[q(z|x)||p(z)]$, which, in an explicit case, is easily computable via
\[KL[q(z|x)||p(z)]=KL[N(\mu_0,\Sigma_0)||N(\mu_1,\Sigma_1)] = \frac{1}{2}(tr(\Sigma_1^{-1}\Sigma_0)+(\mu_1-\mu_0)^T\Sigma_1^{-1}(\mu_1-\mu_0)-k+\log \frac{det \Sigma_1}{det \Sigma_0}\] where k is the dimensionality of the distribution.\\ However, with an implicit $q(z|x)$, we don't have any parameters, only a method of generating samples, so we must find an alternative way of estimating $E_{q(z|x)} \log \frac{q(z|x)}{p(z)}$. Our method of ratio estimation is an adversarial density ratio estimation approach. This involves taking samples from $p(z)$ and $q(z|x)$, labelling the $p(z)$ samples with $y=1$ and the $q(z|x)$ samples with $y=0$, then alternating optimization of a discriminator function $D(z;\theta_d)=P(y=1|z)$, outputting the probability that a sample is from $q(z|x)$, and a generator function $G(\epsilon;\theta_g), \epsilon \sim N(0,I)$, creating $q(z|x)$ samples that imitate $p(z)$. This generator function is different (I think??) to the previous encoder function parameterized by $\phi$. Overall, we want to minimize the Bernoulli loss (other losses can be used):
\begin{align*}
L(\theta_d,\theta_g)&= E_{p(z|y)p(y)}[-y\log D(z;\theta_d)-(1-y)\log (1-D(z;\theta_d))]
\\&= \pi E_{p(z)}[-\log D(z;\theta_d)]+(1-\pi)E_{q(z|x)}[-\log (1-D(z;\theta_d))] \text{ where }\pi = P(y=1)
\\&= \pi E_{p(z)}[-\log D(z;\theta_d)]+(1-\pi)E_{q(z|x)}[-\log (1-D(G(\epsilon ;\theta_g);\theta_d))]
\end{align*}
This is achieved by alternating minimization of the ratio loss and generative loss:
\[ \text{Ratio loss: } \min_{\theta_d} \pi E_{p(z)} [-\log D(x;\theta_d)]+(1-\pi )E_{q(z|x)}[-\log (1-D(z;\theta_d))]\]
\[ \text{Generative loss: } \min_{\theta_g} E_{q(z|x)} [\log (1-D(G(\epsilon ;\theta_g);\theta_d)]\]
The trained discriminator can be used to find the log ratio.\\
The disadvantage of this approach is that $p(z|x$) is not estimated explicitly, making inference very difficult. \\
An alternative approach to estimating $p(z|x)$ with an implicit $q(z|x)$ is to use the adversarial approach on the two distributions directly, but this gives us even less information to perform inference, as we won't have an estimate for the ELBO.\\
Our goal is to find better ways of estimating $log \frac{q(z|x)}{p(z)}$, either improving the speed or accuracy of the computation, or finding a method that provides more information of $p(z|x)$. We will implement the improved log ratio into common implicit variational models such as VAEs and GANs, run these on common benchmarks such as MNIST and ImageNet, then compare the results against current state-of-the-art models.
\newpage
\section{Learning}
\subsection{Variational Inference}
\subsubsection{Context}
In Bayesian statistics, a common problem is to estimate posterior densities, so that we may perform inference to determine an unknown parameter. Consider a set of unknown, latent variables $\textbf{Z}=\{z_i\}^M_{i=1}$ and a dataset of known variables $\textbf{X}=\{x_i\}^N_{i=1}$. These sets have a joint density of $P(\textbf{Z},\textbf{X})$. In the Bayesian framework, inference is often performed on the posterior density (the distribution of the parameters $\textbf{Z}$ after the data $\textbf{X}$ is observed)
$P(\textbf{Z}|\textbf{X})$, which, after applying Bayes' theorem, can be written as:
\begin{equation*}P(\textbf{Z}|\textbf{X})=\frac{P(\textbf{Z})P(\textbf{X}|\textbf{Z})}{P(\textbf{X})}= \frac{P(\textbf{Z})P(\textbf{X}|\textbf{Z})}{\int_\mathbcal{Z}P(\textbf{Z},\textbf{X})d\textbf{Z}}\end{equation*}
where
\begin{itemize}
\item $P(\textbf{Z})$ is the prior distribution: the initial distribution of $\textbf{Z}$ before the data $\textbf{X}$ is observed. This can be initialised to represent our initial beliefs, or it can be parametrised randomly,
\item $P(\textbf{X}|\textbf{Z})$ is the likelihood: the distribution of data $\textbf{X}$ conditioned on the parameters $\textbf{Z}$,
\item $P(\textbf{X})=\int_\mathbcal{Z}P(\textbf{Z},\textbf{X})d\textbf{Z}$ is the marginal likelihood, or the evidence: the density of the data averaged across all possible parameter values.
\end{itemize}
If the evidence integral $P(\textbf{X})=\int_\mathbcal{Z}P(\textbf{Z},\textbf{X})d\textbf{Z}$ is impossible or difficult to compute (possibly because it is unavailable in closed form or the dimensionality is too high), then we are unable to evaluate the posterior density. Traditionally, MCMC(Markov Chain Monte Carlo) methods overcome this obstacle by constructing a Markov chain that converges to the stationary distribution $P(\textbf{Z}|\textbf{X})$, then sampling from the chain to create an empirical estimate for the posterior distribution. However, these methods rely on the speed of convergence, which can be slow for large datasets or complex models. When faced with these issues or when desiring a faster computation, one may instead apply variational inference, an alternative approach to density estimation.
\subsubsection{Introduction to Variational Inference}
Variational inference chooses another distribution $Q(\textbf{Z})$ from a select family of variational distributions (approximate densities) $\mathcal{Q}$ to serve as an approximation to $P(\textbf{Z}|\textbf{X})$, and then minimizes the divergence between the two distributions in an optimization problem:
\begin{equation}
Q^*(\textbf{Z})=\argmin_{Q(\textbf{Z})\in \mathcal{Q}}D(Q(\textbf{Z})||P(\textbf{Z}|\textbf{X}))
\end{equation} where $D$ denotes an f-divergence (a measure of how divergent two probability distributions are, it is minimized if $Q=P$). This results in an analytical approximation to the posterior density. Additionally, a lower bound for the marginal likelihood of the dataset is derived, which can be used as a model selection criterion. Due to the stochastic nature of the optimization, variational inference methods can be much faster than MCMC, but the solution is only locally optimal as there is no guarantee of global convergence.
\subsubsection{Derivation of the ELBO}
The most common f-divergence used in variational inference is the KL(Kullback-Leibler) divergence, defined as the expected logarithmic difference between two distrbutions $Q$ and $P$ with respect to $Q$:
\begin{equation*}
KL(Q||P)=\int_{-\infty}^\infty Q(x)\log \frac{Q(x)}{P(x)}dx=\mathbb{E}_{Q(x)}\left[\log\frac{Q(x)}{P(x)}\right].
\end{equation*}
Note that the KL divergence is not symmetric. We use the reverse KL divergence instead of the forward KL divergence $KL(P||Q)$ because it leads to an expectation maximization algorithm as opposed to an expectation propagation algorithm.\\
Using this expression, we can rewrite equation (1) as:
\begin{align*}
Q^*(\textbf{Z})&=\argmin_{Q(\textbf{Z})\in \mathcal{Q}}KL(Q(\textbf{Z})||P(\textbf{Z}|\textbf{X}))\\
&= \argmin_{Q(\textbf{Z})\in \mathcal{Q}} \mathbb{E}_{Q(\textbf{Z})}[\log Q(\textbf{Z})-\log P(\textbf{Z}|\textbf{X})]\\
&= \argmin_{Q(\textbf{Z})\in \mathcal{Q}} \mathbb{E}_{Q(\textbf{Z})}\left[\log Q(\textbf{Z})-\log\frac{P(\textbf{X}|\textbf{Z})P(\textbf{Z})}{P(\textbf{X})}\right]\\
&= \argmin_{Q(\textbf{Z})\in \mathcal{Q}} \left(\mathbb{E}_{Q(\textbf{Z})}[\log Q(\textbf{Z})-\log P(\textbf{X}|\textbf{Z})-\log P(\textbf{Z})]+\log P(\textbf{X})\right).
\end{align*}
Note in the last line $\mathbb{E}_{Q(\textbf{Z})}[P(\textbf{X})]=P(\textbf{X})$ as it is not dependent on $Q(\textbf{Z})$. Also note that the KL divergence is dependent on $P(\textbf{X})$, which we have determined to be intractable, so this optimization problem cannot be solved in this form. This issue is resolved by rearranging the KL divergence expression as follows:
\begin{align}
KL(Q(\textbf{Z})||P(\textbf{Z}|\textbf{X}))&=\mathbb{E}_{Q(\textbf{Z})}[\log Q(\textbf{Z})-\log P(\textbf{X}|\textbf{Z})-\log P(\textbf{Z})]+\log P(\textbf{X}) \nonumber \\
\log P(\textbf{X})-KL(Q(\textbf{Z})||P(\textbf{Z}|\textbf{X}))&=-\mathbb{E}_{Q(\textbf{Z})}[\log Q(\textbf{Z})-\log P(\textbf{X}|\textbf{Z})-\log P(\textbf{Z})]\nonumber \\
&=\mathbb{E}_{Q(\textbf{Z})}[\log P(\textbf{X}|\textbf{Z})]-\mathbb{E}_{Q(\textbf{Z})}[\log Q(\textbf{Z})-\log P(\textbf{Z})]\nonumber \\
&=\mathbb{E}_{Q(\textbf{Z})}[\log P(\textbf{X}|\textbf{Z})]-KL(Q(\textbf{Z})||P(\textbf{Z})).
\end{align}
We refer to $\log P(\textbf{X})-KL(Q(\textbf{Z})||P(\textbf{Z}|\textbf{X}))$ as $ELBO(Q)$ (evidence lower bound), as it is equal to the marginal probability of the data subtracted by a constant error term. This error term $KL(Q(\textbf{Z})||P(\textbf{Z}|\textbf{X}))$ becomes 0 when $Q(\textbf{Z})=P(\textbf{Z}|\textbf{X})$, maximizing the ELBO. Note that since $P(\textbf{X})$ is constant, maximizing the $ELBO$ is equal to minimizing the KL divergence between $Q(\textbf{Z})$ and $P(\textbf{Z}|\textbf{X})$, and that the expression on line (2) is entirely computable. We can therefore rewrite our optimization problem from equation (1) as:
\begin{align*}
Q^*(\textbf{Z})&=\argmin_{Q(\textbf{Z})\in \mathcal{Q}}D(Q(\textbf{Z})||P(\textbf{Z}|\textbf{X}))\\
&= \argmax_{Q(\textbf{Z})\in \mathcal{Q}} ELBO(Q)\\
&= \argmax_{Q(\textbf{Z})\in \mathcal{Q}} \left(\mathbb{E}_{Q(\textbf{Z})}[\log P(\textbf{X}|\textbf{Z})]-KL(Q(\textbf{Z})||P(\textbf{Z}))\right).
\end{align*}
\subsubsection{Mean-Field Variational Family}
The family of variational distributions $\mathcal{Q}$ is typically a 'mean-field variational family', in which the distribution $Q(\textbf{Z})$ factorizes over the latent variables $\{z_i\}^M_{i=1}$:
\begin{equation}
Q(\textbf{Z})=\prod^M_{i=1}q_i(z_i).
\end{equation}
The individual factors $q_i(z_i)$ can take any form. We want to choose these factors so that $ELBO(Q)$ is maximized. To derive an expression for the optimal factor $q_i^*(z_i)$, we substitute equation (3) into the $ELBO$, factor out a specific $q_j(z_j)$ and equate the functional derivative of the resulting Lagrangian equation with 0. Firstly, we express $ELBO(Q)$ in an integral form as follows:
\begin{align*}
ELBO(Q)&= \mathbb{E}_{Q(Z)}[\log P(\textbf{X}|\textbf{Z})]-KL(Q(\textbf{Z})||P(\textbf{Z}))\\
&= \mathbb{E}_{Q(Z)}[\log P(\textbf{X}|\textbf{Z})+\log P(\textbf{Z})-\log Q(\textbf{Z})]\\
&= \mathbb{E}_{Q(Z)}[\log P(\textbf{X}, \textbf{Z})-\log Q(\textbf{Z})]\\
&= \int_{\mathbcal{Z}}Q(\textbf{Z})(\log P(\textbf{X},\textbf{Z})-\log Q(\textbf{Z}))d\textbf{Z}.
\end{align*}
Substituting $Q(\textbf{Z})=\prod^M_{i=1}q_i(z_i)$ and factoring out $q_j(z_j)$ yields:
\begin{align}
ELBO(Q)&= \int_\mathbcal{Z}\left[\prod^M_{i=1}q_i(z_i)\right]\left(\log P(\textbf{X},\textbf{Z})-\sum_{i=1}^M\log q_i(z_i)\right)d\textbf{Z}\nonumber\\
&= \int_\mathcal{z_j}q_j(z_j)\left(\int_\mathbcal{z_{-j}}\log P(\textbf{X},\textbf{Z})\prod_{i\neq j}q_i(z_i)d\textbf{z}_{-j} \right) dz_j\nonumber\\
&\quad -\int_{\mathcal{z_j}}q_j(z_j)\left(\int_{\mathbcal{z}_{-j}}\left[\prod_{i\neq j}q_i(z_i)\right]\sum_{i=1}^M q_i(z_i)d\textbf{z}_{-j}\right)dz_j\nonumber\\
&= \int_{\mathcal{z_j}}q_j(z_j)\mathbb{E}_{\textbf{z}_{-j}}[\log P(\textbf{X},\textbf{Z})]dz_j-\int_{\mathcal{z_j}}q_j(z_j)\log q_j(z_j)\left(\int_{\mathbcal{z}_{-j}}\prod_{i\neq j}q_i(z_i)dz_{-j}\right) dz_j\nonumber\\
&\quad -\int_{\mathcal{z_j}}q_j(z_j)\left(\int_{\mathbcal{z}_{-j}}\left[\prod_{i\neq j}q_i(z_i)\right]\sum_{i\neq j}q_i(z_i)d{\textbf{z}_{-j}}\right)dz_j\nonumber\\
&= \int_\mathcal{z_j}q_j(z_j)\mathbb{E}_{\textbf{z}_{-j}}[\log P(\textbf{X},\textbf{Z})]dz_j-\int_\mathcal{z_j}q_j(z_j)\log q_j(z_j)dz_j\nonumber\\
&\quad -\int_{\mathbcal{z}_{-j}}\left[\prod_{i\neq j}q_i(z_i)\right]\sum_{i\neq j}q_i(z_i)d{\textbf{z}_{-j}}
\\&= \int_\mathcal{z_j}q_j(z_j)\left(\mathbb{E}_{\mathbcal{z}_{-j}}[\log P(\textbf{X},\textbf{Z})]-\log q_j(z_j)\right)dz_j+\text{const}.
\end{align}
The term in line (4) becomes a constant as it does not depend on $q_j(z_j)$. Now our Lagrangian equation with the constraint that $q_i(z_i)$ are probability density functions is:
\begin{equation*}
ELBO(Q)-\sum^M_{i=1}\lambda_i\int_\mathcal{z_i}q_i(z_i)dz_i=0
\end{equation*}
or using our expression for $ELBO(Q)$ in line (5),
\begin{equation}
\int_\mathcal{z_j}q_j(z_j)\left(\mathbb{E}_{\mathbcal{z}_{-j}}[\log P(\textbf{X},\textbf{Z})]-\log q_j(z_j)\right)dz_j-\sum^M_{i=1}\lambda_i\int_\mathcal{z_i}q_i(z_i)dz_i+\text{const}=0.
\end{equation}
Using the Euler-Lagrange equation (need to put this in), we then take the functional derivative of (6) with respect to $q_j(z_j)$ (in this case, the partial derivative with respect to $q_j(z_j)$ of the expression inside the integral):
\begin{align}
\frac{\partial ELBO(q)}{\partial q_j(z_j)}&= \frac{\partial}{\partial q_j(z_j)}\left[q_j(z_j)\left(\mathbb{E}_{\textbf{z}_{-j}}[\log P(\textbf{X},\textbf{Z})]-\log q_j(z_j)\right)-\lambda_jq_j(z_j)\right]\nonumber
\\&= \mathbb{E}_{\textbf{z}_{-j}}[\log P(\textbf{X},\textbf{Z})]-\log q_j(z_j)-1-\lambda_j
\end{align}
Equating expression (7) to 0 and letting $1+\lambda_j$ be a constant (as it is independent of $z$), we have:
\begin{align*}
\log q_j^*(z_j)&= \mathbb{E}_{\textbf{z}_{-j}}[\log P(\textbf{X},\textbf{Z})]-\text{const}\\
q_j^*(z_j)&=\frac{e^{\mathbb{E}_{\textbf{z}_{-j}}[\log P(\textbf{X},\textbf{Z})]}}{\text{const}}\\
&= \frac{e^{\mathbb{E}_{\textbf{z}_{-j}}[\log P(\textbf{X},\textbf{Z})]}}{\int e^{\mathbb{E}_{\textbf{z}_{-j}}[\log P(\textbf{X},\textbf{Z})]}dz_j}.
\end{align*}
The normalization constant on the denominator can be easily derived by observing $q^*_j(z_j)$ as a density. Lastly, we derive a simpler expression of $q^*_j(z_j)$ by observing that terms independent of $z_j$ can be treated as a constant:
\begin{align}
q^*_j(z_j)&\propto \exp\left(\mathbb{E}_{\textbf{z}_{-j}}[\log P(\textbf{X},\textbf{Z})]\right)\nonumber\\
&\propto \exp\left(\mathbb{E}_{\textbf{z}_{-j}}[\log P(z_j|\textbf{z}_{-j},\textbf{X})]\right).
\end{align}
This expression can be used in an expectation-maximization algorithm, in which the $q^*_j(z_j)$ is evaluated and iterated from $j=1\dots M$. This particular algorithm is called coordinate ascent variational inference (CAVI) (Algorithm 1):\\
\\
\begin{algorithm}[H]
\caption{Coordinate Ascent Variational Inference (CAVI)}
\KwData{Dataset $\textbf{X}$ and Model P($\textbf{X},\textbf{Z}$)}
\KwResult{Approximate density $Q(\textbf{Z})=\prod^M_{i=1}q_i(z_i)$}
\BlankLine
\Begin{
Initialize random variational factors $q_j(z_j)$\;
\While{ELBO(Q) has not converged}{

	\For{$j=1$ \KwTo $m$}{
	Set $q_j(z_j)\propto \exp(\mathbb{E}[\log P(z_j|\textbf{z}_{-j},\textbf{X})])$\;
	}
	Calculate $ELBO(Q)=\mathbb{E}[\log P(\textbf{Z},\textbf{X})]-\mathbb{E}[\log Q(\textbf{Z})]$\;
}
Return $Q(\textbf{Z})$\;
}
\end{algorithm}
\subsubsection{Example: Bayesian mixture of Gaussians}
To illustrate the variational inference approach, we will use the Bayesian mixture of Gaussians example from (Blei, 2018/16 idk).\\
Consider the hierarchical model
\begin{align*}
\mu_k&\sim N(0,\sigma^2), &&k=1,\dots,K,\\
c_i&\sim \text{Categorical}\left(\frac{1}{K},\dots,\frac{1}{K}\right), &&i=1,\dots,n,\\
x_i|c_i,\bm{\mu}&\sim N(c^\top_i\bm{\mu},1), &&i=1,\dots,n.
\end{align*}
This is a Bayesian mixture of univariate Gaussian random variables with unit variance. In this model, we draw $K$ $\mu_k$ variables from a prior Gaussian distribution $N(0,\sigma^2)$ ($\sigma^2$ is a hyperparameter), forming the vector $\bm{\mu}=(\mu_1,\dots,\mu_K)^\top$. We then generate an indicator vector $c_i$ of length $K$ from a prior categorical distribution. This vector has zeros for every element except for one element, where it is a $1$. Each element has equal probability $1/K$ of being the element that contains the $1$. The transpose of this $c_i$ is then multiplied by $\bm{\mu}$, essentially choosing one of the $\bm{\mu}$ elements at random. We then draw $x_i$ from the resulting $N(c^\top_i\bm{\mu},1)$.\\
Here, our latent variables are $\textbf{z}=\{\textbf{c},\bm{\mu}\}$. Assuming $n$ samples, our joint density is
\begin{equation}
p(\bm{\mu},\textbf{c},\textbf{x})=p(\bm{\mu})\prod^n_{i=1}p(c_i)p(x_i|c_i, \bm{\mu}).\end{equation}
From this, we derive the marginal likelihood
\[p(\textbf{x})=\int p(\bm{\mu})\prod^n_{i=1}\sum_{c_i}p(c_i)p(x_i|c_i,\bm{\mu})d\bm{\mu}.\]
This integral is intractable, as the time complexity of evaluating it is $\mathcal{O}(K^n)$, which is exponential in $K$. To evaluate the posterior distribution over the latent variables $p(\bm{\mu},\textbf{c}|\textbf{x})$, we would have to apply variational inference, approximating it with a variational distribution $q(\bm{\mu},\textbf{c})$. We will assume this distribution follows the mean-field variational family:
\[q(\bm{\mu},\textbf{c})=\prod^K_{k=1}q(\mu_k;m_k,s^2_k)\prod^n_{i=1}q(c_i;\bm{\phi_i}).\]
In this distribution, we have $K$ Gaussian factors with mean $\mu_k$ and variance $s^2_k$, and $n$ categorical factors with index probabilities defined by the vector $\bm{\phi_i}$, such that
\begin{align*}
\mu_k&\sim N(m_k,s^2_k), &&k=1,\dots,K,\\
x_i&\sim \text{Categorical}(\bm{\phi_i}), &&i=1,\dots,n.
\end{align*}
Using this and equation (9), we can derive $ELBO(q)$:
\begin{align*}
ELBO(q)&=\mathbb{E}[\log p(\textbf{z},\textbf{x})]-\mathbb{E}[\log q(\textbf{z})]\\
&=\mathbb{E}[\log p(\bm{\mu,c},\textbf{x})]-\mathbb{E}[\log q(\bm{\mu,c})]\\
&=\sum^K_{i=1}\mathbb{E}[\log p(\mu_k); m_k,s^2_k]+\sum^k_{i=1}\left(\mathbb{E}[\log p(c_i);\bm{\phi}_i]+\mathbb{E}[\log p(x_i|c_i,\bm{\mu});\bm{\phi}_i,\textbf{m},\textbf{s}^2]\right)\\
&\quad -\sum^K_{k=1}\mathbb{E}[\log q(\mu_k;m_k,s^2_k)]-\sum^n_{i=1}\mathbb{E}[\log q(c_i;\bm{\phi}_i)]
\end{align*}
From equation (8), we derive the optimal categorical factor
\begin{equation}q^*(c_i;\bm{\phi}_i)\propto \exp\left(\log p(c_i)+\mathbb{E}[\log p(x_i|c_i,\bm{\mu});\textbf{m},\textbf{s}^2]\right).\end{equation}
Now since $c_i$ is an indicator vector,
\[p(x_i|c_i,\bm{\mu})=\prod^K_{k=1}p(x_i|\mu_k)^{c_{ik}}.\]
We can now evaluate the second term of equation (10):
\begin{align*}
\mathbb{E}\left([\log p(x_i|c_i,\bm{\mu});\textbf{m},\textbf{s}^2]\right)&=\sum_{k=1}^K c_{ik}\mathbb{E}[\log p(x_i|\mu_k);m_k,s^2_k]\\
&=\sum_{k=1}^K c_{ik}\mathbb{E}[-(x_i-\mu_k)^2/2;m_k,s^2_k]+\text{const}\\
&=\sum_{k=1}^Kc_{ik}\left(\mathbb{E}[\mu_k;m_k,s^2_k]x_i-\mathbb{E}[\mu^2_k;m_k,s^2_k]/2\right)+\text{const}.
\end{align*}
In each line, terms constant with respect to $c_{ik}$ have been taken out of the expression. By proportionality, we have the variational update
\[\phi_{ik}\propto \exp\left(\mathbb{E}[\mu_k;m_k,s^2_k]x_i-\mathbb{E}[\mu^2_k;m_k,s^2_k]/2\right).\]
\begin{algorithm}
\caption{CAVI Algorithm for Bayesian mixture of Gaussians}
\KwData{Data $\textbf{x}$, Number of Gaussian components $K$, Hyperparameter value $\sigma^2$}
\KwResult{Optimal variational factors $q(\mu_k;m_k,s^2_k)$ and $q(c_i;\bm{\phi_i)}$}
\BlankLine
\Begin{
Randomly initialize parameters $\textbf{m}, \textbf{s}^2$ and $\bm{\phi}$\;
\While{ELBO has not converged}{
	\For{$i=1$ \KwTo $n$}{
		Set $\phi_{ik}\propto\exp\left(\mathbb{E}[\mu_k;m_k,s^2_k]x_i-\mathbb{E}			[\mu^2_k;m_k,s^2_k]/2\right)$\;
	}
	\For{$k=1$ \KwTo $K$}{
		Set $m_k=\frac{\sum_i\phi_{ik}x_i}{1/ \sigma^2+\sum_i\phi_{ik}}$\;
		Set $s^2_k=\frac{1}{1/ \sigma^2+\sum_i \phi_{ik}}$\;
	}
	Compute $ELBO(\textbf{m},\textbf{s}^2,\bm{\phi})$\;
}
Return $q(\textbf{m},\textbf{s}^2,\bm{\phi})$\;
}
\end{algorithm}
\subsubsection{References}
To be organised properly and moved to the end later:\\
$https://en.wikipedia.org/wiki/Variational_Bayesian_methods$ \\
http://bjlkeng.github.io/posts/variational-bayes-and-the-mean-field-approximation/ \\
https://arxiv.org/pdf/1601.00670.pdf \\
Pattern recognition and machine learning by Bishop (2006) pages 461-dunno \\
https://www.cs.cmu.edu/~epxing/Class/10708-17/notes-17/10708-scribe-lecture13.pdf \\
\newpage
\subsection{Neural Networks}
\subsubsection{Motivation}
To discuss brief history, relation to neuroscience.
\subsubsection{Overall Structure}
Insert picture, discuss input output layers, bias/intercept nodes
\subsubsection{Node Structure}
internal structure of node, activation function e.g. sigmoid or tanh
\subsubsection{Back-Propagation}
Give algorithm for training neural network.
\subsubsection{Example: MNIST Classification}
Show exact pseudocode algorithm for classifying MNIST samples with results/pictures. (I have already coded a basic NNet MNIST classifier at home).
\newpage
\subsection{Autoencoding Variational Bayes}
Kingma 2013
\subsubsection{Problem Context}
Introduce notations, similar to VB but with NN parametrization. Discuss problems with intractability and large dataset, introduce the 3 main things we want to solve.
\subsubsection{Structure}
Diagram of VAE
\subsubsection{Variational Bound}
Take ELBO from VB section but parametrize it with NN, discuss the typical useless Monte Carlo gradient estimator.
\subsubsection{Algorithm}
Propose better estimator of lower bound and its derivatives, derive AEVB algorithm.
\subsubsection{Reparametrization Trick}
Explain the trick and show changes in structure.
\subsubsection{Example: Variational Autoencoder (for MNIST? or cats :) or human faces D: )}
Show pseudocode and diagram for VAE (if different from before) and show output from own VAE code (gonna have to code my own VAE).
\newpage
\subsection{Density Ratio Estimation}
Mohamed 2016, Sugiyama textbook, Goodfellow 2014, Huszar 2017
\subsubsection{Context: Implicit Generative Models}
Describe these models and the inference on them
\subsubsection{Loss Functions}
Choice and derivation of loss functions
\subsubsection{Class Probability Estimation}
Adversarial Classifier, algorithm
\subsubsection{Divergence Minimisation}

\subsubsection{Ratio Matching}

\subsubsection{Moment Matching}

\subsubsection{Denoising}

\newpage
\subsection{Likelihood-Free Variational Inference}
Tran 2017 (This section may go before Density Ratio Estimation)
\subsubsection{Context}
Lack of likelihood introduces additional intractability...
\subsubsection{Variational Bound}
Similar to VB section but expression is very different and there is alot of intractability.
\subsubsection{Ratio Estimation}
Derive loss function and minimize it, estimating the ratio in the process
\subsubsection{Algorithm}
Put everything together in an algorithm (LFVI)
\end{document}
